[{"authors":null,"categories":null,"content":"I am a PhD student in the research group of Prof. Dr.-Ing. Eckehard Steinbach at the Technical University of Munich (TUM). I studied Electrical and Computer Engineering at TUM for both Bachelor and Master. From 2016 to 2018, I was a software engineer at the Objective Software GmbH and Luxoft Inc and worked in cooperation with the BMW Group in the area of Automotive and Autonomous Driving. In January 2019, I joined the Chair of Media Technology at the Technical University of Munich as a research and teaching associate and PhD candidate.\nMy current research is focused on video processing, compression, and transmission of multi-camera systems for autonomous and teleoperated driving.\n","date":1625961600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1625961600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a PhD student in the research group of Prof. Dr.-Ing. Eckehard Steinbach at the Technical University of Munich (TUM). I studied Electrical and Computer Engineering at TUM for both Bachelor and Master.","tags":null,"title":"Markus Hofbauer","type":"authors"},{"authors":["Christopher Kuhn","Markus Hofbauer","Goran Petrovic","Eckehard Steinbach"],"categories":null,"content":"","date":1625961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625961600,"objectID":"37e488fe5a5711ad5c6642846e5a820c","permalink":"https://hofbi.github.io/publication/introspection_trajectory/","publishdate":"2021-05-30T00:00:00Z","relpermalink":"/publication/introspection_trajectory/","section":"publication","summary":"In autonomous driving, complex traffic scenarios can cause situations that require human supervision to resolve safely. Instead of only reacting to such events, it is desirable to predict them early in advance. While predicting the future is challenging, there is a source of information about the future readily available in autonomous driving: the planned trajectory the car intends to drive. In this paper, we propose to analyze the trajectories planned by the vehicle to predict failures early on. We consider sequences of trajectories and use machine learning to detect patterns that indicate impending failures. Since no public data of disengagements of autonomous vehicles is available, we use data provided by development vehicles of the BMW Group. From over six months of test drives, we obtain more than 2600 disengagements of the automated system. We train a Long Short-Term Memory classifier with sequences of planned trajectories that either resulted in successful driving or disengagements. The proposed approach outperforms existing state-of-the-art failure prediction with low-dimensional data by more than 3% in a Receiver Operating Characteristic analysis. Since our approach makes no assumptions on the underlying system, it can be applied to predict failures in other safety-critical areas of robotics as well.","tags":["Failure Prediction","Autonomous Driving"],"title":"Trajectory-Based Failure Prediction for Autonomous Driving","type":"publication"},{"authors":["Markus Hofbauer"],"categories":null,"content":"","date":1623081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623081600,"objectID":"18d3bc2cdbb3a83cd292d4d7af8e683d","permalink":"https://hofbi.github.io/talk/scale-your-workflow/","publishdate":"2021-06-07T00:00:00Z","relpermalink":"/talk/scale-your-workflow/","section":"event","summary":"Introduction to different tools and methods for an efficient, scalable, and maintainable workflow.","tags":["Software Engineering","Research"],"title":"Scale your Workflow","type":"event"},{"authors":null,"categories":null,"content":"Selected topics in media technology. The focus is on current research topics and new technologies. The participants study recent publications, prepare a summary in the form of a scientific paper and present the topic to the audience.\nDetails at the chairs webpage\nSupervised Terms  SS21 WS20/21 SS20 WS19/20 SS19  SS21: Augmented Reality and Virtual Reality  AR/VR-based Human-Robot Interfaces VR and AR Interfaces for Robot Learning from Demonstration Intuitive Teleoperation using Augmented Reality Improving Teleoperated Driving Using an Augmented Reality Representation Deep Learning for 6D pose estimation and its applications in AR Deformable Object Tracking for AR Marker-based Augmented Reality Visual Place Recognition Activity Recognition for Augmented Reality and Virtual Reality Augmented and Virtual Haptics Generation of Realistic Virtual Views Video coding optimization of virtual reality 360-degree Video  WS20/21: Recent Advances in Multimedia Compression - From 3D Point Clouds to Deep Neural Networks  Compression Schemes for 360-degree Video Streaming Compression Artifacts Reduction for Compressed Image/Videos End-to-End Image/Video Compression With Neural Networks Semantic Image/Video Compression Through Deep Learning Video Compression for Low Delay Video Streaming Point Cloud Compression for Autonomous Driving Map Compression for Visual SLAM Systems Deep Autoencoder Model for Dynamically Acquired Point Cloud Data Compression Enhancement Using IMU Sensors Compression of Pose Graphs for SLAM systems Vibrotactile Perception and Compression Neural Network Model Compression Knowledge Distillation for Enhanced Neural Networks  SS20: Recent Advances in Multimedia Compression - From 3D Point Clouds to Deep Neural Networks  Compression Schemes for 360-degree Video Streaming Compression Artifacts Reduction for Compressed Image/Videos End-to-End Image/Video Compression With Neural Networks Semantic Image/Video Compression Through Deep Learning Video Compression for Low Delay Video Streaming Point Cloud Compression for Autonomous Driving Map Compression for Visual SLAM Systems Deep Autoencoder Model for Dynamically Acquired Point Cloud Data Compression Enhancement Using IMU Sensors Log Compression for Robotics Vibrotactile Perception and Compression Neural Network Model Compression Knowledge Distillation for Enhanced Neural Networks  WS19/20: Autonomous Driving Methods  Teleoperation for Autonomous Driving Failures Safety Concepts for Teleoperated Driving Early Failure Prediction in Autonomous Driving Out-Of-Distribution Detection for Autonomous Driving Visual SLAM Methods Path Planning Driver intention/motion prediction Personalized Autonomous Driving: Learning from Driver Demonstrations Multi-modal Object Detection Predictive Methods for Network Delay Compensation by Teledriving Vision Enhancement for Autonomous Driving under Adverse Weather Conditions Self-supervised Learning of Depth Estimation and Ego-Motion from Monocular Videos using Convolutional Neural Networks  SS19: Machine Learning  Human Visual Perception as a Model for Neural Networks Dimension Reduction - Extracting Information from High-Dimensional Data Embedded Deep Learning for Sensor Fusion Applications Throughput Prediction in Cellular Networks Region of Interest Prediction for Teleoperated Driving Applications Quality of Experience Prediction in Mobile Video Image Quality Assessment Image Restoration - from Conventional Methods to Neural Networks Approaches Content-Based Image Retrieval Machine Learning for Path Planning Neural SLAM Tactile Object Description and Exploration  ","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"1058b4200950b77d187b2fe44d84a3f7","permalink":"https://hofbi.github.io/teaching/seminar-media-technology/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/teaching/seminar-media-technology/","section":"teaching","summary":"Selected topics in media technology. The focus is on current research topics and new technologies. The participants study recent publications, prepare a summary in the form of a scientific paper and present the topic to the audience.","tags":null,"title":"Seminar Media Technology","type":"Master Course (MSEI)"},{"authors":["Markus Hofbauer","Christoph Bachhuber","Kevin Meyer","Eckehard Steinbach"],"categories":null,"content":"Basic introduction to principles of software engineering including unit testing, test driven development, refactoring and more.\nDescription Software engineering skills of any engineer become more important with the ongoing digitization and automation. Software engineering means not only delivering an implementation that fulfills the requirements and generates correct results. A second highly important aspect is to deliver software that can be maintained by multiple contributors, over months, years or even decades. This lifetime aspect and the number of developers contributing to the software introduces additional requirements and challenges in the development process. Software has to be testable, readable, and extendable. Concepts such as unit tests, code reviews, and continuous integration support writing software with these characteristics.\nDetails at the chairs webpage\nSupervised Years  SS21  Acknowledgement I learned a lot while creating this course at the Chair of Media Technology. I want to thank Christoph Bachhuber and Kevin Meyer for their contributions and discussions while working on this course. I also want to thank Prof. Eckehard Steinbach for giving me the freedom and support in creating and running this course.\n","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"477ad129ac127835bba81483b5b8944d","permalink":"https://hofbi.github.io/teaching/software-lab/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/teaching/software-lab/","section":"teaching","summary":"Basic introduction to principles of software engineering including unit testing, test driven development, refactoring and more.\nDescription Software engineering skills of any engineer become more important with the ongoing digitization and automation.","tags":null,"title":"Software Engineering Lab","type":"Bachelor Course (BSEI)"},{"authors":["Christopher Kuhn","Markus Hofbauer","Goran Petrovic","Eckehard Steinbach"],"categories":null,"content":"","date":1608422400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608422400,"objectID":"a07c3eb4a1f536c6b52f39dc5b888234","permalink":"https://hofbi.github.io/publication/introspection_bb_fusion/","publishdate":"2020-12-20T00:00:00Z","relpermalink":"/publication/introspection_bb_fusion/","section":"publication","summary":"We present an introspective failure prediction approach for autonomous vehicles. In autonomous driving, complex or unknown scenarios can cause a disengagement of the self-driving system. Disengagements can be triggered either by automatic safety measures or by human intervention. We propose to use recorded disengagement sequences from test drives as training data to learn to predict future failures. The system then learns introspectively from its own previous mistakes. In order to predict failures as early as possible, we propose a machine learning approach where sequences of sensor data are classified as either failure or success. The car itself is treated as a black box. Our method combines two sensor modalities that contain different types of information. An image-based model learns to detect generally challenging situations such as crowded intersections accurately multiple seconds in advance. A state data based model allows to detect fast changes immediately before a failure, such as sudden braking or swerving. The outcome of the individual models is fused by averaging the individual failure probabilities. We evaluate our approach on a data set provided by the BMW Group containing 14 hours of autonomous driving. The proposed late fusion approach allows for predicting failures at an accuracy of more than 85% seven seconds in advance, at a false positive rate of 20%. The proposed method outperforms state-of-the-art failure prediction by more than 15% while being a flexible framework that allows for straightforward addition of further sensor modalities.","tags":["Teleoperated Driving","Failure Prediction"],"title":"Introspective Failure Prediction for Autonomous Driving Using Late Fusion of State and Camera Information","type":"publication"},{"authors":["Markus Hofbauer","Christopher Kuhn","Goran Petrovic","Eckehard Steinbach"],"categories":null,"content":"","date":1606867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606867200,"objectID":"c8df0bba50685bc70dd8f71a2450ac41","permalink":"https://hofbi.github.io/publication/single_encoder/","publishdate":"2020-10-12T00:00:00Z","relpermalink":"/publication/single_encoder/","section":"publication","summary":"Teleoperated driving (TOD) is a possible solution to cope with failures of autonomous vehicles. In TOD, the human operator perceives the traffic situation via video streams of multiple cameras from a remote location. Adaptation mechanisms are needed in order to match the available transmission resources and provide the operator with the best possible situation awareness. This includes the adjustment of individual camera video streams according to the current traffic situation. The limited video encoding hardware in vehicles requires the combination of individual camera frames into a larger superframe video. While this enables the encoding of multiple camera views with a single encoder, it does not allow for rate/quality adaptation of the individual views. To this end, we propose a novel concept that uses preprocessing filters to enable individual rate/quality adaptations in the superframe video. The proposed preprocessing filters allow for the usage of existing multidimensional adaptation models in the same way as for individual video streams using multiple encoders. Our experiments confirm that the proposed concept is able to control the spatial, temporal and quality resolution of individual segments in the superframe video. Additionally, we demonstrate the usability of the proposed method by applying it in a multi-view teledriving scenario. We compare our approach to individually encoded video streams and a multiplexing solution without preprocessing. The results show that the proposed approach produces bitrates for the individual video streams which are comparable to the bitrates achieved with separate encoders. While achieving a similar bitrate for the most important views, our approach requires a total bitrate that is 40% smaller compared to the multiplexing approach without preprocessing.","tags":["Teleoperated Driving","Adaptive Streaming","C++","ROS"],"title":"Adaptive Multi-View Live Video Streaming for Teledriving Using a Single Hardware Encoder","type":"publication"},{"authors":["Christopher Kuhn","Markus Hofbauer","Goran Petrovic","Eckehard Steinbach"],"categories":null,"content":"","date":1606867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606867200,"objectID":"8a2d5378301ccded77b95a1026e91dbf","permalink":"https://hofbi.github.io/publication/blt/","publishdate":"2020-11-09T00:00:00Z","relpermalink":"/publication/blt/","section":"publication","summary":"Accurate visual scene perception plays an important role in fields such as medical imaging or autonomous driving. Recent advances in computer vision allow for accurate image classification, object detection and even pixel-wise semantic segmentation. Human vision has repeatedly been used as an inspiration for developing new machine vision approaches. In this work, we propose to adapt the \"zoom lens model\" from psychology for semantic scene segmentation. According to this model, humans first distribute their attention evenly across the entire field of view at low processing power. Then, they follow visual cues to look at a few smaller areas with increased attention. By looking twice, it is possible to refine the initial scene understanding without requiring additional input. We propose to perform semantic segmentation the same way. To obtain visual cues for deciding where to look twice, we use a failure region prediction approach based on a state-of-the-art failure prediction method. Then, the second, focused look is performed by a dedicated classifier that reclassifies the most challenging patches. Finally, pixels predicted to be errors are updated in the original semantic prediction. While focusing only on areas with the highest predicted failure probability, we achieve a classification accuracy of over 63% for the predicted failure regions. After updating the initial semantic prediction of 4000 test images from a large-scale driving data set, we reduce the absolute pixel-wise error of 232 road participants by 10% or more.","tags":["Computer Vision","Failure Prediction"],"title":"Better Look Twice - Improving Visual Scene Perception Using a Two-Stage Approach","type":"publication"},{"authors":["Markus Hofbauer","Christopher Kuhn","Lukas Püttner","Goran Petrovic","Eckehard Steinbach"],"categories":null,"content":"","date":1606867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606867200,"objectID":"a008b06e2bcfb1d40aaf2852d88bc442","permalink":"https://hofbi.github.io/publication/driver_sa/","publishdate":"2020-10-12T00:00:00Z","relpermalink":"/publication/driver_sa/","section":"publication","summary":"With increasing progress in autonomous driving, the human does not have to be in control of the vehicle for the entire drive. A human driver obtains the control of the vehicle in case of an autonomous system failure or when the vehicle encounters an unknown traffic situation it cannot handle on its own. A critical part of this transition to human control is to ensure a sufficient driver situation awareness. Currently, no direct method to explicitly estimate driver awareness exists. In this paper, we propose a novel system to explicitly measure the situation awareness of the driver. Our approach is inspired by methods used in aviation. However, in contrast to aviation, the situation awareness in driving is determined by the detection and understanding of dynamically changing and previously unknown situation elements. Our approach uses machine learning to define the best possible situation awareness. We also propose to measure the actual situation awareness of the driver using eye tracking. Comparing the actual awareness to the target awareness allows us to accurately assess the awareness the driver has of the current traffic situation. To test our approach, we conducted a user study. We measured the situation awareness score of our model for 8 unique traffic scenarios. The results experimentally validate the accuracy of the proposed driver awareness model.","tags":["Situation Awareness","Eye Tracking","ROI Prediction","Python","ROS"],"title":"Measuring Driver Situation Awareness Using Region-of-Interest Prediction and Eye Tracking","type":"publication"},{"authors":["Christopher Kuhn","Markus Hofbauer","Goran Petrovic","Eckehard Steinbach"],"categories":null,"content":"","date":1603152000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603152000,"objectID":"01d95e0b0092f7f0359859ef605ba25d","permalink":"https://hofbi.github.io/publication/introspection_bb/","publishdate":"2020-04-20T00:00:00Z","relpermalink":"/publication/introspection_bb/","section":"publication","summary":"Failures in autonomous driving caused by complex traffic situations or model inaccuracies remain inevitable in the near future. While much research is focused on how to prevent such failures, comparatively little research has been done on predicting them. An early failure prediction would allow for more time to take actions to resolve challenging situations. In this work, we propose an introspective approach to predict future disengagements of the car by learning from previous disengagement sequences. Our method is designed to detect failures as early as possible by using sensor data from up to ten seconds before each disengagement. The car itself is treated as a black box, with only its state data and the number of detected objects being required. Since no model-specific knowledge is needed, our method is applicable to any self-driving system. Currently, no public data of real-life disengagements is available. To test our approach, we therefore use autonomous driving data provided by BMW that was collected with BMW research vehicles over three months. We show that an LSTM classifier trained with sequences of state data can predict failures up to seven seconds in advance with an accuracy of more than 80%. This is two seconds earlier than comparable approaches from the literature.","tags":["Teleoperated Driving","Failure Prediction"],"title":"Introspective Black Box Failure Prediction for Autonomous Driving","type":"publication"},{"authors":["Markus Hofbauer","Christopher Kuhn","Goran Petrovic","Eckehard Steinbach"],"categories":null,"content":"","date":1603152000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603152000,"objectID":"deb4bae991a7276cca7457fbe768acf4","permalink":"https://hofbi.github.io/publication/telecarla/","publishdate":"2020-04-20T00:00:00Z","relpermalink":"/publication/telecarla/","section":"publication","summary":"Teledriving is a possible fallback mode to cope with failures of fully autonomous vehicles. One important requirement for teleoperated vehicles is a reliable low delay data transmission solution, which adapts to the current network conditions to provide the operator with the best possible situation awareness. Currently, there is no easily accessible solution for the evaluation of such systems and algorithms in a fully controllable environment available. To this end we propose an open source framework for teleoperated driving research using low-cost off-the-shelf components. The proposed system is an extension of the open source simulator CARLA, which is responsible for rendering the driving environment and providing reproducible scenario evaluation. As a proof of concept, we evaluated our teledriving solution against CARLA in remote and local driving scenarios. The proposed teledriving system leads to almost identical performance measurements for local and remote driving. In contrast, remote driving using CARLA's client server communication results in drastically reduced operator performance. Further, the framework provides an interface for the adaptation of the temporal resolution and target bitrate of the compressed video streams. The proposed framework reduces the required setup effort for teleoperated driving research in academia and industry.","tags":["Teleoperated Driving","Adaptive Streaming","C++","ROS","CARLA"],"title":"TELECARLA: An Open Source Extension of the CARLA Simulator for Teleoperated Driving Research Using Off-The-Shelf Components","type":"publication"},{"authors":null,"categories":null,"content":"The major goals of the seminar are to learn how to do scientific research and to learn and practice presentation techniques. Each student has to prepare a scientific talk about the topic he or she has registered for. The students have to collect the required literature, understand its contents, and prepare a presentation about it that summarizes the topic.\nDetails at the chairs webpage\nSupervised Terms  WS20/21 WS19/20  WS20/21: Recent Advances in Multimedia Compression - From 3D Point Clouds to Deep Neural Networks  Compression Schemes for 360-degree Video Streaming Compression Artifacts Reduction for Compressed Image/Videos End-to-End Image/Video Compression With Neural Networks Semantic Image/Video Compression Through Deep Learning Video Compression for Low Delay Video Streaming Point Cloud Compression for Autonomous Driving Map Compression for Visual SLAM Systems Deep Autoencoder Model for Dynamically Acquired Point Cloud Data Compression Enhancement Using IMU Sensors Compression Schemes for Cloud Robotics Vibrotactile Perception and Compression Neural Network Model Compression Knowledge Distillation for Enhanced Neural Networks  Subjects WS19/20: Autonomous Driving Methods The planned topics for this semester:\n Teleoperation for Autonomous Driving Failures Early Failure Prediction in Autonomous Driving Visual SLAM Methods Path Planning Attentive Sensory Data Rate Adaptation Multi-modal Object Detection Predictive Methods for Network Delay Compensation by Teledriving Vision Enhancement for Autonomous Driving under Adverse Weather Conditions  ","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"82e574aca2b6d5db27a0d930bbec1cef","permalink":"https://hofbi.github.io/teaching/ssp/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/teaching/ssp/","section":"teaching","summary":"The major goals of the seminar are to learn how to do scientific research and to learn and practice presentation techniques. Each student has to prepare a scientific talk about the topic he or she has registered for.","tags":null,"title":"Seminar on Topics in Signal Processing","type":"Master Course (MSEI)"},{"authors":["Christopher Kuhn","Markus Hofbauer","Sungkyu Lee","Goran Petrovic","Eckehard Steinbach"],"categories":null,"content":"","date":1600560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600560000,"objectID":"e66b785d5eab97e233228d72e724d80a","permalink":"https://hofbi.github.io/publication/introspection_seg/","publishdate":"2020-05-08T00:00:00Z","relpermalink":"/publication/introspection_seg/","section":"publication","summary":"Semantic segmentation of images enables pixel-wise scene understanding which in turn is a critical component for tasks such as autonomous driving. While recent implementations of semantic image segmentation have achieved remarkable accuracy, misclassifications remain inevitable. For safety-critical tasks such as free-space computing, it is desirable to know when and where the segmentation will fail. We propose using the concept of introspection to predict the failures of a given semantic segmentation model. A separate introspective model is trained to predict the errors of a given model. This is accomplished by training the given model with the errors made on a set of previous inputs. By using the same architecture for the introspective model as for the semantic segmentation, the proposed model learns to predict pixel-wise failure probabilities. This allows to predict both when and where the semantic segmentation will fail. Sharing the feature encoder with the inspected model reduces training and inference time while improving performance. We evaluate our approach on the large-scale A2D2 driving data set. In a precision-recall analysis, the proposed method outperforms two state-of-the-art uncertainty estimation methods by 3.2% and 6.7% while requiring significantly less resources during inference. Additionally, combining introspection with a state-of-the-art method further increases the performance by up to 3.7%.","tags":["Teleoperated Driving","Failure Prediction"],"title":"Introspective Failure Prediction for Semantic Image Segmentation","type":"publication"},{"authors":["Markus Hofbauer","Christopher Kuhn","Jiaming Meng","Goran Petrovic","Eckehard Steinbach"],"categories":null,"content":"","date":1600560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600560000,"objectID":"b5a785f1e9c2d860f6e0235b32079936","permalink":"https://hofbi.github.io/publication/mvroi/","publishdate":"2020-05-08T00:00:00Z","relpermalink":"/publication/mvroi/","section":"publication","summary":"Visual environment perception is one of the key elements for autonomous and manual driving. Modern fully automated vehicles are equipped with a range of different sensors and capture the surroundings with multiple cameras. The ability to predict human driver's attention is the basis for various autonomous driving functions. State-of-the-art attention prediction approaches use only a single front facing camera and rely on automatically generated training data. In this paper, we present a manually labeled multi-view region of interest dataset. We use our dataset to finetune a state-of-the-art region of interest prediction model for multiple camera views. Additionally, we show that using two separate models focusing on either front or rear view data improves the region of interest prediction. We further propose a semi-supervised annotation framework which uses the best performing finetuned models for generating pseudo labels to improve the efficiency of the labeling process. Our results show that existing region of interest prediction performs well on front view data, but finetuning improves the performance especially for rear view data. Our current dataset consists of about 16000 images and we plan to further increase the size of the dataset. The dataset and the source code of the proposed semi-supervised annotation framework will be made available on GitHub and can be used to generate custom region of interest data.","tags":["Teleoperated Driving","ROI Prediction","Python","CARLA"],"title":"Multi-View Region of Interest Prediction for Autonomous Driving Using Semi-Supervised Labeling","type":"publication"},{"authors":["Markus Hofbauer"],"categories":null,"content":"","date":1593782100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593782100,"objectID":"858bc533775f8c3ffba7508cb430b36e","permalink":"https://hofbi.github.io/talk/doctoral-seminar-adaptive-streaming-of-sensor-information-for-teleoperator-situation-awareness/","publishdate":"2020-07-03T13:15:00Z","relpermalink":"/talk/doctoral-seminar-adaptive-streaming-of-sensor-information-for-teleoperator-situation-awareness/","section":"event","summary":"This talk gives an overview of the status in the research project \"Adaptive Streaming of Sensor Information for Teleoperator Situation Awareness","tags":["Adaptive Streaming","Teleoperated Driving","Situation Awareness"],"title":"Doctoral Seminar - Adaptive Streaming of Sensor Information for Teleoperator Situation Awareness","type":"event"},{"authors":["Markus Hofbauer"],"categories":["Software Engineering","Research"],"content":"Scale your Workflow  Overview  Project Management Development Documentation   Project Management Integrated in GitLab|GitHub|Bitbucket|\u0026hellip;\n Issue Trackers Milestones Boards \u0026hellip;   Issue Trackers  Collect ideas Document and discuss your decisions/progress  For/with others For/with your future self      Team Discussions  Early feedback Constantly explain/justify/rethink your ideas  Fail Often, Fail Fast, Fail Early\n Development  Implementation  Common rules you should always follow  SOLID Guidelines DRY (Don\u0026rsquo;t repeat yourself)   Design Patterns  Existing solutions for common design problems Don\u0026rsquo;t reinvent the wheel     Testing  Unit Tests  Test individual components   Integration Tests  Test interaction of larger parts   System Tests  Test the entire system     Checks  Consistency is key Automate whenever possible Code Format  Consistent code layout Available for almost every language   Linters  Avoid common errors     C++  Clang-Format CMake-Format Clang-Tidy CppCheck CppLint   Python  black yapf pylint flake8   Sample for t, m, ms in bag.read_messages(): # do something  C0103: Variable name \u0026quot;t\u0026quot; doesn't conform to snake_case naming style (invalid-name) W0612: Unused variable 't' (unused-variable)   Tools Know your tools\n IDE for the heavy development Editor as your swiss army knife Shortcuts   Version Control  Git as most common tool Git Feature Branch Workflow Code Review  Knowledge transfer in both directions     Continuous Integration  Automatically run in a configured environment Requires scripted/containerized environment setup Avoid \u0026ldquo;Works on my machine\u0026rdquo;   CI Failure  CI Pass  Documentation  Summarize the latest status Requires active maintenance   Questions? ","date":1588345200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588345200,"objectID":"e32f427afb26b4135bb7f36c85285d3e","permalink":"https://hofbi.github.io/slides/workflow/","publishdate":"2020-05-01T15:00:00Z","relpermalink":"/slides/workflow/","section":"slides","summary":"Introduction to different tools and methods for an efficient, scalable, and maintainable workflow.","tags":["Software Engineering","Research"],"title":"Scale your Workflow","type":"slides"},{"authors":["Markus Hofbauer"],"categories":null,"content":"","date":1579186800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579186800,"objectID":"e1f90235a06be2a7dbfe58eb420feff5","permalink":"https://hofbi.github.io/talk/test-driven-development/","publishdate":"2020-01-16T15:00:00Z","relpermalink":"/talk/test-driven-development/","section":"event","summary":"General Introduction to Unit Tests and Test Driven Development by the example of C++.","tags":["C++","TDD","Software Engineering"],"title":"Test Driven Development","type":"event"},{"authors":["Markus Hofbauer"],"categories":["Software Engineering"],"content":"Test Driven Development Code\n Unit Tests  Separate (non production) program/code to test your code Test your code on the lowest (unit) layer General 3 step structure of a unit test  Given: Setup unit and environment When: Execute unit to test Then: Test for expected result     Why Tests Michael Feathers, Working Effectively with Legacy Code\n “To me, legacy code is simply code without tests.”\n  TDD  Write your test before your production code 3 Phases  Red Phase: Define a failing test Green Phase: Fix that test (Solve Simple) Refactoring: Clean up your code      FizzBuzz Task Write a function that returns the number it was given or Fizz if it is multiple of 3, Buzz if it is multiple of 5 or FizzBuzz if it is multiple of both.\nC++ | Python\n Task Definition  Can call function fizzBuzz Return 1 for 1 Return 2 for 2 Return Fizz for 3 Return Buzz for 5 Return Fizz for 6 Return Buzz for 10 Return FizzBuzz for 15   Questions? Try yourself\n","date":1579186800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579186800,"objectID":"0cec0527be2bf6b6cf102dd838004dac","permalink":"https://hofbi.github.io/slides/tdd/","publishdate":"2020-01-16T15:00:00Z","relpermalink":"/slides/tdd/","section":"slides","summary":"General Introduction to Unit Tests and Test Driven Development","tags":["C++","SOLID","Software Engineering"],"title":"Test Driven Development","type":"slides"},{"authors":["Markus Hofbauer"],"categories":null,"content":" S - Single-responsiblity principle. O - Open-closed principle. L - Liskov substitution principle. I - Interface segregation principle. D - Dependency Inversion Principle.  ","date":1573138800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573138800,"objectID":"afe18e0a195cbd861faf656216cd9cbc","permalink":"https://hofbi.github.io/talk/solid-principles/","publishdate":"2019-11-07T15:00:00Z","relpermalink":"/talk/solid-principles/","section":"event","summary":"General Introduction to the SOLID Principles at the example of C++.","tags":["C++","SOLID","Software Engineering"],"title":"SOLID Principles","type":"event"},{"authors":null,"categories":null,"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"646a32e7a7c67888dc107f9af41df101","permalink":"https://hofbi.github.io/project/mucplusplus/","publishdate":"2019-09-01T00:00:00Z","relpermalink":"/project/mucplusplus/","section":"project","summary":"Member of the C++ User Group Munich","tags":["Software Engineering","C++"],"title":"MUC++","type":"project"},{"authors":["Markus Hofbauer","Christopher Kuhn","Eckehard Steinbach"],"categories":null,"content":"","date":1563494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563494400,"objectID":"e06d030dedad113c71f57e898725cdb3","permalink":"https://hofbi.github.io/publication/coc19/","publishdate":"2019-07-19T00:00:00Z","relpermalink":"/publication/coc19/","section":"publication","summary":"","tags":["Teleoperated Driving","Adaptive Streaming","Failure Prediction","CARLA","ROS"],"title":"Teleoperation for Autonomous Driving Failures","type":"publication"},{"authors":null,"categories":null,"content":"The practical module teaches fundamental methods of communication related to Industry 4.0 applications across 6 topical elements, each taught by a different member of the CoC Communication.\nDetails at the chairs webpage\nSupervised Years  2019  ","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554076800,"objectID":"20511b362cb295b362ad7f93f4ac58e8","permalink":"https://hofbi.github.io/teaching/i40-lab/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/teaching/i40-lab/","section":"teaching","summary":"The practical module teaches fundamental methods of communication related to Industry 4.0 applications across 6 topical elements, each taught by a different member of the CoC Communication.\nDetails at the chairs webpage","tags":null,"title":"Industry 4.0 Laboratory","type":"Bachelor Course (BSEI)"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://hofbi.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"351d61fd8e32fbe631b4feb21e5a28bf","permalink":"https://hofbi.github.io/project/carla/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/project/carla/","section":"project","summary":"Open-source simulator for autonomous driving research","tags":["Autonomous Driving","Research","C++","Python"],"title":"CARLA","type":"project"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"840a5607cf0c6809be660526d6934c31","permalink":"https://hofbi.github.io/project/research/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/project/research/","section":"project","summary":"Adaptive Streaming of Sensor Information for Teleoperator Situation Awareness","tags":["Research","Teleoperated Driving","Adaptive Streaming"],"title":"PhD Research","type":"project"},{"authors":null,"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"bfc0ba37d6bbb04e93710f7c50236d19","permalink":"https://hofbi.github.io/project/makeapp/","publishdate":"2014-01-01T00:00:00Z","relpermalink":"/project/makeapp/","section":"project","summary":"Overview of private and open source projects","tags":["Software Engineering","C++","Python","Flutter","Kotlin","Swift"],"title":"MaKeApp","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://hofbi.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]